---
# Role: benchmark_runner
# Description: Executes gas benchmarks for a single client using run.sh script

- name: Log benchmark execution phase
  ansible.builtin.debug:
    msg: "========== Benchmark Execution Phase: {{ client_name }} =========="

# =============================================================================
# Pre-Execution Cleanup
# Stops containers, removes volumes, cleans execution-data directory
# =============================================================================
- name: Clean up previous execution state
  ansible.builtin.shell: |
    set -e
    CLIENT="{{ client_name }}"
    CLIENT_DIR="{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/scripts/${CLIENT}"

    # Stop and remove existing containers
    cd "$CLIENT_DIR"
    docker compose down --volumes --remove-orphans 2>/dev/null || true

    # Remove Docker volumes (check .env for custom name, plus common patterns)
    volume_name=$(grep -E "^EC_VOLUME_NAME=" "$CLIENT_DIR/.env" 2>/dev/null | cut -d'=' -f2 || true)
    [ -n "$volume_name" ] && docker volume rm "$volume_name" 2>/dev/null || true
    docker volume rm "${CLIENT}_execution_data" 2>/dev/null || true
    docker volume rm "gas-${CLIENT}_execution_data" 2>/dev/null || true

    # Clean execution-data directory
    execution_data_dir="$CLIENT_DIR/execution-data"
    if [ -d "$execution_data_dir" ]; then
      rm -rf "$execution_data_dir"/*
      echo "cleaned"
    else
      mkdir -p "$execution_data_dir"
      echo "created"
    fi
  args:
    executable: /bin/bash
  become: yes
  register: cleanup_result
  changed_when: "'cleaned' in cleanup_result.stdout"
  failed_when: false

# Clean stale genesis/temp files from /tmp (needed fresh for each client)
# These can become directories if script crashes mid-copy, causing permission errors
- name: Clean stale temp files
  ansible.builtin.file:
    path: "{{ item }}"
    state: absent
  loop:
    - /tmp/chainspec.json
    - /tmp/genesis.json
  become: yes
  failed_when: false

# Clean JWT secret path (prevents "cannot create regular file" errors if it exists as directory)
- name: Clean JWT secret path
  ansible.builtin.file:
    path: "{{ benchmark_jwt_path }}"
    state: absent
  become: yes
  failed_when: false

# =============================================================================
# Nethermind Image Patching
# Non-performance images don't support certain flags. We use yq to cleanly
# remove them from docker-compose.yaml and restore from git after run.
# =============================================================================
- name: Check if Nethermind needs patching
  ansible.builtin.set_fact:
    nethermind_needs_patch: "{{ client_name == 'nethermind' and benchmark_images.nethermind is defined and 'performance' not in benchmark_images.nethermind }}"

- name: Patch Nethermind docker-compose for non-performance images (using yq)
  when: nethermind_needs_patch | default(false)
  ansible.builtin.shell: |
    # Remove performance-only flags from nethermind command array
    # - --Blocks.CachePrecompilesOnBlockProcessing (only in performance images)
    # - --Init.GenesisHash (only in performance images)
    yq -i 'del(.services.nethermind.command[] | select(test("CachePrecompilesOnBlockProcessing|Init\.GenesisHash")))' \
      "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/scripts/nethermind/docker-compose.yaml"
  args:
    executable: /bin/bash
  changed_when: true

# =============================================================================
# Benchmark Execution
# =============================================================================
- name: Execute benchmark for {{ client_name }}
  block:
    - name: Debug - show variables being used
      ansible.builtin.debug:
        msg:
          - "benchmark_test_paths: {{ benchmark_test_paths | default('UNDEFINED') }}"
          - "genesis: {{ benchmark_test_paths[0].genesis | default('UNDEFINED') }}"
          - "filter: {{ benchmark_filter | default('UNDEFINED') }}"
          - "Command will be: bash run.sh -t {{ benchmark_test_paths[0].path | default('') | regex_replace('^' + benchmark_tests_root + '/', '') }} {{ '-g ' + benchmark_test_paths[0].genesis if benchmark_test_paths[0].genesis is defined else '(no genesis)' }} -c {{ client_name }} -r {{ benchmark_runs }} {{ '-f ' + benchmark_filter if benchmark_filter is defined and benchmark_filter != '' else '(no filter)' }}"

    - name: Build and execute benchmark command
      ansible.builtin.shell: |
        # Activate virtualenv (created by environment_setup in the source dir)
        VENV_PATH="{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/.venv"
        if [ -d "$VENV_PATH" ]; then
          source "$VENV_PATH/bin/activate"
          echo "Activated venv at $VENV_PATH"
          echo "Python: $(which python3)"
        else
          echo "ERROR: venv not found at $VENV_PATH"
          echo "Run the environment_setup role first or ensure 'setup' tag is included"
          exit 1
        fi

        # Extract test path (strip benchmark_tests_root prefix if present)
        TEST_PATH="{{ benchmark_test_paths[0].path | regex_replace('^' + benchmark_tests_root + '/', '') }}"

        # Log file for debugging (timestamped directory for no collision)
        LOG_DIR="{{ benchmark_workspace_root }}/{{ benchmark_logs_dir }}/{{ benchmark_run_timestamp }}"
        LOG_FILE="$LOG_DIR/{{ client_name }}_run.log"
        mkdir -p "$LOG_DIR"

        # Run benchmark and capture output to log file
        bash run.sh \
          -t "$TEST_PATH" \
          {{ '-g "' + benchmark_test_paths[0].genesis + '"' if benchmark_test_paths[0].genesis is defined else '' }} \
          -c "{{ client_name }}" \
          -r {{ benchmark_runs }} \
          {{ '-w "' + benchmark_warmup_file + '"' if (benchmark_warmup_file is defined and benchmark_warmup_file != '') else '' }} \
          {{ '-f "' + benchmark_filter + '"' if (benchmark_filter is defined and benchmark_filter != '') else '' }} \
          {{ '-n "' + benchmark_network + '"' if (benchmark_network is defined and benchmark_network != '') else '' }} \
          {{ '-B "' + benchmark_snapshot_root + '"' if (benchmark_overlay_enabled | default(false)) else '' }} \
          {{ '-R' if benchmark_restart_before_testing | default(false) else '' }} \
          {{ '-F' if benchmark_skip_forkchoice | default(false) else '' }} \
          {{ '-o ' + (benchmark_opcodes_warmup_count | string) if (benchmark_opcodes_warmup_count is defined and benchmark_opcodes_warmup_count | int > 0) else '' }} \
          {{ "-i '" + (benchmark_images | to_json) + "'" if (benchmark_images | default({}) | length > 0) else '' }} \
          2>&1 | tee "$LOG_FILE"

        # Return the exit code of bash run.sh (not tee)
        exit ${PIPESTATUS[0]}
      args:
        chdir: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}"
        executable: /bin/bash
      environment:
        JWT_PATH: "{{ benchmark_jwt_path }}"
        PYTHONPATH: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}"
        PATH: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/.venv/bin:{{ ansible_env.PATH }}"
      register: run_result
      changed_when: true
      async: 7200
      poll: 30


    - name: Verify benchmark succeeded
      ansible.builtin.assert:
        that: run_result.rc == 0
        fail_msg: |
          Benchmark failed for {{ client_name }} (exit code: {{ run_result.rc }})

          STDERR: {{ run_result.stderr | default('(none)') }}
          STDOUT (last 20 lines): {{ (run_result.stdout | default('')).split('\n')[-20:] | join('\n') }}

          Check: docker logs $(docker ps -aq --filter name={{ client_name }} | head -1)
        success_msg: "âœ“ Benchmark succeeded for {{ client_name }}"

  rescue:
    - name: Log benchmark failure
      ansible.builtin.debug:
        msg: "Benchmark execution failed for {{ client_name }}"

    - name: Fail with error details
      ansible.builtin.fail:
        msg: "Benchmark execution failed for {{ client_name }}"

  always:
    # =========================================================================
    # Post-Execution: Restore Patched Files
    # =========================================================================
    - name: Restore Nethermind docker-compose.yaml from git
      ansible.builtin.command:
        cmd: git checkout scripts/nethermind/docker-compose.yaml
        chdir: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}"
      when: nethermind_needs_patch | default(false)
      changed_when: true
      failed_when: false

    # =========================================================================
    # Post-Execution: Log Collection
    # =========================================================================
    - name: Collect Docker logs
      ansible.builtin.shell: |
        log_dir="{{ benchmark_workspace_root }}/{{ benchmark_logs_dir }}/{{ benchmark_run_timestamp }}/{{ client_name }}"
        mkdir -p "$log_dir"

        containers=$(docker ps -a --filter "name={{ client_name }}" --format "{% raw %}{{.Names}}{% endraw %}" 2>/dev/null || true)
        count=0
        for container in $containers; do
          docker logs "$container" > "$log_dir/${container}.log" 2>&1 && count=$((count + 1))
        done
        echo "Collected $count container logs to $log_dir"
      args:
        executable: /bin/bash
      register: log_collection
      changed_when: false
      failed_when: false

    # =========================================================================
    # Post-Execution: Validate Reports (optional)
    # =========================================================================
    - name: Validate reports generated
      when: benchmark_validate_reports | default(true)
      block:
        - name: Check reports directory
          ansible.builtin.find:
            paths: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/reports"
            file_type: file
          register: reports_check
          failed_when: false

        - name: Fail if no reports generated
          ansible.builtin.fail:
            msg: |
              {% if reports_check.failed | default(false) %}
              Reports directory does not exist at {{ benchmark_source_dir }}/reports/
              The benchmark script likely crashed before generating output.
              {% else %}
              Reports directory exists but is empty.
              Test filter "{{ benchmark_filter | default('(none)') }}" may have matched zero tests.
              {% endif %}

              TROUBLESHOOTING:
              1. Check Docker logs: docker logs $(docker ps -aq --filter name={{ client_name }} | head -1)
              2. Check benchmark logs: cat {{ benchmark_workspace_root }}/{{ benchmark_logs_dir }}/{{ benchmark_run_timestamp }}/{{ client_name }}_run.log
              3. Verify disk space: df -h {{ benchmark_workspace_root }}
          when: (reports_check.matched | default(0)) == 0

    # =========================================================================
    # Post-Execution: Artifact Synchronization
    # Syncs to timestamped directories: {results,reports,logs}/{{ timestamp }}/
    # =========================================================================
    - name: Ensure timestamped artifact directories exist
      ansible.builtin.file:
        path: "{{ benchmark_workspace_root }}/{{ item }}/{{ benchmark_run_timestamp }}"
        state: directory
        mode: '0755'
      loop:
        - results
        - reports
      loop_control:
        label: "{{ item }}/{{ benchmark_run_timestamp }}"

    - name: Sync artifacts to timestamped directories
      ansible.builtin.synchronize:
        src: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/{{ item }}/"
        dest: "{{ benchmark_workspace_root }}/{{ item }}/{{ benchmark_run_timestamp }}/"
        mode: push
        delete: no
        recursive: yes
      delegate_to: "{{ inventory_hostname }}"
      register: sync_results
      failed_when: false
      loop:
        - results
        - reports
      loop_control:
        label: "{{ item }}/{{ benchmark_run_timestamp }}"

    - name: Log artifact sync summary
      ansible.builtin.debug:
        msg: "Artifacts synced to {{ benchmark_run_timestamp }}/: {{ sync_results.results | selectattr('changed') | map(attribute='item') | list | join(', ') | default('none') }}"
