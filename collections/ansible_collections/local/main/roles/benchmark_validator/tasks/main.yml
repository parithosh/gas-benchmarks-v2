---
# Role: benchmark_validator
# Description: Validates prerequisites and configuration before benchmark execution

- name: Log prerequisite validation phase
  ansible.builtin.debug:
    msg: "========== Prerequisite Validation Phase =========="

# =============================================================================
# Lock File Management
# =============================================================================
- name: Handle lock file (check stale, fail if active, create new)
  ansible.builtin.shell: |
    set -e
    LOCK_FILE="{{ benchmark_lock_file }}"
    MAX_AGE_SECONDS=7200  # 2 hours - matches benchmark async timeout

    if [ -f "$LOCK_FILE" ]; then
      # Get lock file age in seconds (compatible with both Linux and macOS)
      if stat --version >/dev/null 2>&1; then
        LOCK_MTIME=$(stat -c %Y "$LOCK_FILE")
      else
        LOCK_MTIME=$(stat -f %m "$LOCK_FILE")
      fi

      CURRENT_TIME=$(date +%s)
      LOCK_AGE=$((CURRENT_TIME - LOCK_MTIME))

      if [ "$LOCK_AGE" -lt "$MAX_AGE_SECONDS" ]; then
        echo "Benchmark already running (lock file age: ${LOCK_AGE}s, max: ${MAX_AGE_SECONDS}s)" >&2
        exit 1
      fi

      echo "Removing stale lock file (age: ${LOCK_AGE}s)"
      rm -f "$LOCK_FILE"
    fi

    echo "timestamp: $(date -Iseconds)" > "$LOCK_FILE"
  args:
    executable: /bin/bash
  changed_when: true

# =============================================================================
# Configuration Validation
# =============================================================================
- name: Validate overlay configuration
  ansible.builtin.fail:
    msg: "benchmark_snapshot_root must be set when benchmark_overlay_enabled is true"
  when:
    - benchmark_overlay_enabled | default(false)
    - benchmark_snapshot_root | default('') == ''

# Normalize benchmark_clients early (handles string vs list from CLI)
- name: Normalize benchmark_clients to list
  ansible.builtin.set_fact:
    benchmark_clients_normalized: "{{ benchmark_clients | replace(\"'\", '\"') | from_json if benchmark_clients is string else benchmark_clients }}"

# =============================================================================
# Tool Version Validation
# Validates: Docker, Docker Compose, Python, jq, yq, dotnet
# =============================================================================
- name: Check tool versions
  ansible.builtin.command: "{{ item.cmd }}"
  register: tool_versions
  changed_when: false
  failed_when: tool_versions.rc != 0
  loop:
    - { name: "docker", cmd: "docker --version", min: "20.10", regex: '([0-9]+\.[0-9]+)' }
    - { name: "docker_compose", cmd: "docker compose version", min: "2.0", regex: 'v?([0-9]+\.[0-9]+)' }
    - { name: "python", cmd: "python3 --version", min: "3.10", regex: '([0-9]+\.[0-9]+)' }
    - { name: "jq", cmd: "jq --version", min: null, regex: null }
    - { name: "yq", cmd: "yq --version", min: null, regex: null }
    - { name: "dotnet", cmd: "dotnet --version", min: "{{ min_dotnet_version }}", regex: '([0-9]+\.[0-9]+)' }
  loop_control:
    label: "{{ item.name }}"

- name: Parse and validate tool versions
  ansible.builtin.set_fact:
    "{{ item.item.name }}_version": "{{ item.stdout | regex_search(item.item.regex, '\\1') | first }}"
  when: item.item.regex is not none
  loop: "{{ tool_versions.results }}"
  loop_control:
    label: "{{ item.item.name }}"

- name: Fail if tool versions are too old
  ansible.builtin.fail:
    msg: "{{ item.item.name }} version too old (found: {{ lookup('vars', item.item.name + '_version') }}, required: >= {{ item.item.min }})"
  when:
    - item.item.min is not none
    - item.item.regex is not none
    - lookup('vars', item.item.name + '_version') is version(item.item.min, '<')
  loop: "{{ tool_versions.results }}"
  loop_control:
    label: "{{ item.item.name }}"

- name: Log tool versions validated
  ansible.builtin.debug:
    msg: "✓ Tools validated: Docker {{ docker_version }}, Compose {{ docker_compose_version }}, Python {{ python_version }}, dotnet {{ dotnet_version }}"

# =============================================================================
# Docker Runtime Validation
# =============================================================================
- name: Validate Docker daemon is running
  ansible.builtin.command: docker ps
  changed_when: false
  failed_when: false
  register: docker_daemon_result

- name: Fail if Docker daemon is not running
  ansible.builtin.fail:
    msg: "Docker daemon is not running. Start Docker and try again."
  when: docker_daemon_result.rc != 0

# Check if benchmark ports are available (8545: JSON-RPC, 8551: Engine API)
- name: Check if benchmark ports are available
  ansible.builtin.wait_for:
    port: "{{ item }}"
    state: stopped
    timeout: 1
  failed_when: false
  register: port_check_results
  loop: [8545, 8551]

- name: Warn if benchmark ports are in use
  ansible.builtin.debug:
    msg: "⚠️  WARNING: Ports 8545 or 8551 may be in use. This could cause conflicts during benchmarking."
  when: port_check_results.results | selectattr('failed', 'defined') | selectattr('failed') | list | length > 0

# =============================================================================
# Disk Space Validation
# =============================================================================
- name: Check and validate disk space
  ansible.builtin.shell: |
    SPACE_KB=$(df -k {{ benchmark_workspace_root }} | tail -1 | awk '{print $4}')
    SPACE_GB=$((SPACE_KB / 1024 / 1024))
    echo "$SPACE_GB"
  register: disk_space_result
  changed_when: false

- name: Fail if disk space is critically low
  ansible.builtin.fail:
    msg: "Insufficient disk space ({{ disk_space_result.stdout }}G available, minimum 5G required)"
  when: disk_space_result.stdout | int < 5

- name: Warn if disk space is low
  ansible.builtin.debug:
    msg: "⚠️  WARNING: Low disk space ({{ disk_space_result.stdout }}G). Benchmarks may require 10G+."
  when: disk_space_result.stdout | int < 10

# =============================================================================
# Benchmark Files Validation
# =============================================================================
# Required files: run.sh (executable), setup_node.py, requirements.txt
- name: Validate required benchmark files
  ansible.builtin.stat:
    path: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/{{ item }}"
  register: required_files
  loop:
    - "run.sh"
    - "setup_node.py"
    - "requirements.txt"

- name: Fail if required benchmark files are missing or run.sh not executable
  ansible.builtin.fail:
    msg: >-
      {% if not item.stat.exists %}
      Required file not found: {{ benchmark_source_dir }}/{{ item.item }}
      {% elif item.item == 'run.sh' and not item.stat.executable %}
      {{ benchmark_source_dir }}/run.sh exists but is not executable
      {% endif %}
  when: not item.stat.exists or (item.item == 'run.sh' and not item.stat.executable)
  loop: "{{ required_files.results }}"
  loop_control:
    label: "{{ item.item }}"

# =============================================================================
# Client Configuration Validation
# Validates: client directory exists, docker-compose.yaml exists
# =============================================================================
- name: Validate client configurations
  ansible.builtin.stat:
    path: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/scripts/{{ item.0 }}/{{ item.1 }}"
  register: client_config_stats
  loop: "{{ benchmark_clients_normalized | product(['', 'docker-compose.yaml']) | list }}"
  loop_control:
    label: "{{ item.0 }}/{{ item.1 | default('directory', true) }}"

- name: Fail if client configuration is missing
  ansible.builtin.fail:
    msg: >-
      {% if item.item.1 == '' %}
      Client directory not found: scripts/{{ item.item.0 }}
      {% else %}
      docker-compose.yaml not found for client {{ item.item.0 }}
      {% endif %}
  when: not item.stat.exists
  loop: "{{ client_config_stats.results }}"
  loop_control:
    label: "{{ item.item.0 }}/{{ item.item.1 | default('directory', true) }}"

# =============================================================================
# Test Path Validation
# =============================================================================
- name: Validate test paths exist
  ansible.builtin.stat:
    path: "{{ benchmark_workspace_root }}/{{ item.path }}"
  register: test_path_stats
  loop: "{{ benchmark_test_paths }}"
  loop_control:
    label: "{{ item.path }}"

- name: Fail if test path not found
  ansible.builtin.fail:
    msg: "Test path not found: {{ item.item.path }}"
  when: not item.stat.exists
  loop: "{{ test_path_stats.results }}"
  loop_control:
    label: "{{ item.item.path }}"

# =============================================================================
# Warmup File Validation (optional)
# =============================================================================
- name: Validate warmup file exists (if specified)
  when: benchmark_warmup_file is defined and benchmark_warmup_file != ""
  block:
    - name: Check warmup file
      ansible.builtin.stat:
        path: "{{ benchmark_workspace_root }}/{{ benchmark_warmup_file }}"
      register: warmup_file_stat

    - name: Fail if warmup file not found
      ansible.builtin.fail:
        msg: "Warmup file not found: {{ benchmark_warmup_file }}"
      when: not warmup_file_stat.stat.exists

# =============================================================================
# Genesis File Validation
# Note: besu/nethermind use client-specific genesis, others use geth format
# =============================================================================
- name: Build genesis client mapping
  ansible.builtin.set_fact:
    genesis_client_map: "{{ genesis_client_map | default({}) | combine({item: ('geth' if item not in ['besu', 'nethermind'] else item)}) }}"
  loop: "{{ benchmark_clients_normalized }}"

- name: Validate genesis files exist
  ansible.builtin.stat:
    path: "{{ benchmark_workspace_root }}/{{ benchmark_source_dir }}/scripts/genesisfiles/{{ genesis_client_map[item.0] }}/{{ item.1.genesis }}"
  register: genesis_file_stats
  loop: "{{ benchmark_clients_normalized | product(benchmark_test_paths | selectattr('genesis', 'defined') | list) | list }}"
  loop_control:
    label: "{{ item.0 }} -> {{ genesis_client_map[item.0] }}/{{ item.1.genesis }}"
  when: item.1.genesis is defined

- name: Fail if genesis file not found
  ansible.builtin.fail:
    msg: "Genesis file not found: {{ genesis_client_map[item.item.0] }}/{{ item.item.1.genesis }} (for client {{ item.item.0 }})"
  when: item.stat is defined and not item.stat.exists
  loop: "{{ genesis_file_stats.results | default([]) }}"
  loop_control:
    label: "{{ item.item.0 | default('') }} -> {{ item.item.1.genesis | default('') }}"

# =============================================================================
# Validation Summary
# =============================================================================
- name: Log validation summary
  ansible.builtin.debug:
    msg:
      - "✓ All prerequisites validated successfully"
      - "  Clients: {{ benchmark_clients_normalized | join(', ') }}"
      - "  Test paths: {{ benchmark_test_paths | map(attribute='path') | join(', ') }}"

- name: Register validation results
  ansible.builtin.set_fact:
    validation_results:
      docker_version: "{{ docker_version }}"
      docker_compose_version: "{{ docker_compose_version }}"
      python_version: "{{ python_version }}"
      dotnet_version: "{{ dotnet_version }}"
      clients_validated: "{{ benchmark_clients_normalized }}"
      test_paths_validated: "{{ benchmark_test_paths | map(attribute='path') | list }}"
