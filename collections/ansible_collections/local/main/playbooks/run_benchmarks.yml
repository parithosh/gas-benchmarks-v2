---
- name: Ansible Benchmark Orchestration
  hosts: all
  gather_facts: yes

  vars:
    client_failures: []
    playbook_start_time: "{{ ansible_date_time.epoch }}"
    local_artifacts_dir: "{{ playbook_dir }}/../../../../../"

  tasks:
    - name: Record playbook start time and run timestamp
      ansible.builtin.set_fact:
        playbook_start_time: "{{ ansible_date_time.epoch }}"
        benchmark_run_timestamp: "{{ ansible_date_time.iso8601_basic_short }}"
      tags: ['always']

    - name: Log run timestamp
      ansible.builtin.debug:
        msg: "Benchmark run timestamp: {{ benchmark_run_timestamp }}"
      tags: ['always']

    # Normalize benchmark_clients (handles string from CLI -e flag)
    # Note: Extra vars have highest precedence and can't be overridden, so we use a new variable
    - name: Normalize benchmark_clients to list
      ansible.builtin.set_fact:
        _benchmark_clients_list: "{{ benchmark_clients | replace(\"'\", '\"') | from_json if benchmark_clients is string else benchmark_clients }}"
      tags: ['always']

    # Phase 1: Environment Setup
    - name: Environment setup
      ansible.builtin.include_role:
        name: environment_setup
      vars:
        benchmark_workspace_root: "{{ workspace_root }}"
      tags: ['setup']

    # Phase 2: Validation
    - name: Validate prerequisites
      ansible.builtin.include_role:
        name: benchmark_validator
      vars:
        benchmark_workspace_root: "{{ workspace_root }}"
      tags: ['always']

    # Phase 3: Benchmark Execution (sequential, per client)
    - name: Run benchmarks for each client
      block:
        - name: Execute benchmarks for client
          ansible.builtin.include_role:
            name: benchmark_runner
          vars:
            client_name: "{{ item }}"
            benchmark_workspace_root: "{{ workspace_root }}"
          loop: "{{ benchmark_clients }}"
          loop_control:
            loop_var: item
          register: benchmark_results
          ignore_errors: yes

        - name: Track failed clients
          ansible.builtin.set_fact:
            client_failures: "{{ client_failures + [item.item] }}"
          loop: "{{ benchmark_results.results | default([]) }}"
          loop_control:
            loop_var: item
          when: item.failed | default(false)
          no_log: true

      tags: ['benchmarks']

    # Phase 3.5: Fetch Artifacts (remote execution only, after all clients)
    - name: Fetch artifacts to local machine
      when: inventory_hostname not in ['localhost', '127.0.0.1']
      tags: ['benchmarks']
      block:
        - name: Ensure local artifact directories exist
          ansible.builtin.file:
            path: "{{ local_artifacts_dir }}/{{ item }}"
            state: directory
            mode: '0755'
          delegate_to: localhost
          loop:
            - results
            - reports
            - logs

        - name: Fetch artifacts from remote (pull only, no overwrite)
          ansible.posix.synchronize:
            src: "{{ workspace_root }}/{{ item }}/"
            dest: "{{ local_artifacts_dir }}/{{ item }}/"
            mode: pull
            recursive: yes
            rsync_opts:
              - "--ignore-existing"
          delegate_to: localhost
          register: fetch_results
          failed_when: false
          loop:
            - results
            - reports
            - logs

        - name: Log remote fetch summary
          ansible.builtin.debug:
            msg: "Remote artifacts fetched to {{ local_artifacts_dir }}/: {{ fetch_results.results | selectattr('changed') | map(attribute='item') | list | join(', ') | default('none') }}"

    # Phase 4: PostgreSQL Ingestion (optional, tag-based)
    - name: Publish results to PostgreSQL
      ansible.builtin.include_role:
        name: postgres_publisher
      tags: ['postgres']
      when: postgres_host | default('') != ''

  post_tasks:
    # Phase 5: Cleanup and Final Status
    - name: Calculate execution time
      ansible.builtin.set_fact:
        playbook_end_time: "{{ ansible_date_time.epoch }}"
        execution_duration: "{{ (ansible_date_time.epoch | int) - (playbook_start_time | int) }}"
      tags: ['always']

    - name: Cleanup tasks (always run)
      block:
        - name: Remove lock file
          ansible.builtin.file:
            path: "{{ benchmark_lock_file | default('/tmp/gas_benchmark.lock') }}"
            state: absent

        - name: Clean up temp files (JWT, genesis)
          ansible.builtin.file:
            path: "{{ item }}"
            state: absent
          loop:
            - "{{ benchmark_jwt_path | default('/tmp/jwtsecret') }}"
            - /tmp/genesis.json
            - /tmp/chainspec.json
          become: yes

        - name: Clean up overlay mounts
          ansible.builtin.shell: |
            # Find and unmount all overlay mounts under overlay-runtime
            if [ -d overlay-runtime ]; then
              for mount in $(mount | grep overlay-runtime | awk '{print $3}'); do
                umount -l "$mount" 2>/dev/null || true
              done
              # Remove overlay-runtime directory
              rm -rf overlay-runtime
            fi
          args:
            executable: /bin/bash
          when: benchmark_overlay_enabled | default(false)
          ignore_errors: yes
      always:
        - name: Ensure cleanup completion
          ansible.builtin.debug:
            msg: "Cleanup tasks completed"
      tags: ['always']

    - name: Fail playbook if any client failed
      ansible.builtin.fail:
        msg: "One or more clients failed during benchmark execution: {{ client_failures | join(', ') }}"
      when: client_failures | length > 0
      tags: ['always']
